{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN(PyTorch) - MNIST Transpose Convolutional Autoencoder\n",
    "\n",
    "Steps to perform Autoencoder\n",
    "- Input data is passed through an encoder \n",
    "- Encoder will compress the input\n",
    "- Compressed data is is passed through a decoder to reconstruct the input data\n",
    "\n",
    "\n",
    "Note: In general, the encoder and decoder will be built with neural networks, then trained on example data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2293c21e828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating Convolutional Autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to perform Autoencoder\n",
    "- Input data is passed through an encoder\n",
    "    - Input images are 28x28x1 in size. these images will be passed through encoder layers\n",
    "- Encoder will compress the input\n",
    "    - It will have convolutional layers followed by max pooling layer to reduce dimensions to 7x7x4\n",
    "- Compressed data is is passed through a decoder to reconstruct the input data\n",
    "    - This layer will bring back to original dimension 28x28x1\n",
    "    - Will use transposed convolutional layers to increase width and height of compressed input\n",
    "    - Transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. so we will be using stride = 2 and kernel_size=2\n",
    "\n",
    "Note: The images from this dataset are already normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- With Autoencodes we dont need to check for labels because we will be comparing the original input to decoded output\n",
    "- Because we are comparing pixel values in input and output images, we will use loss function for regession task. We will use MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 0.543737\n",
      "Epoch: 2 Training Loss: 0.268778\n",
      "Epoch: 3 Training Loss: 0.246844\n",
      "Epoch: 4 Training Loss: 0.231655\n",
      "Epoch: 5 Training Loss: 0.218430\n",
      "Epoch: 6 Training Loss: 0.208734\n",
      "Epoch: 7 Training Loss: 0.202445\n",
      "Epoch: 8 Training Loss: 0.197927\n",
      "Epoch: 9 Training Loss: 0.194492\n",
      "Epoch: 10 Training Loss: 0.191992\n",
      "Epoch: 11 Training Loss: 0.190074\n",
      "Epoch: 12 Training Loss: 0.188551\n",
      "Epoch: 13 Training Loss: 0.187113\n",
      "Epoch: 14 Training Loss: 0.185937\n",
      "Epoch: 15 Training Loss: 0.184871\n",
      "Epoch: 16 Training Loss: 0.183643\n",
      "Epoch: 17 Training Loss: 0.182225\n",
      "Epoch: 18 Training Loss: 0.181051\n",
      "Epoch: 19 Training Loss: 0.179974\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "for e in range(1, epochs+1):\n",
    "    train_loss = 0.0  # monitor training loss\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        images, _ = data                        # we are just intrested in just images\n",
    "        # no need to flatten images\n",
    "        optimizer.zero_grad()                   # clear the gradients\n",
    "        outputs = model(images)                 # forward pass: compute predicted outputs \n",
    "        loss = criterion(outputs, images)       # calculate the loss\n",
    "        loss.backward()                         # backward pass\n",
    "        optimizer.step()                        # perform optimization step\n",
    "        train_loss += loss.item()*images.size(0)# update running training loss\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {}'.format(e),\n",
    "          '\\tTraining Loss: {:.4f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing inputs before encoding and output after decoding\n",
    "\n",
    "Below we can notice that ouput edges are not smooth like in inputs. This is due to checkerboard effect that will happen with transpose layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets get batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images = images.numpy()                    # prep images for display\n",
    "output = model(images)                     # get sample outputs\n",
    "output = output.view(batch_size, 1, 28, 28)# resizing output\n",
    "output = output.detach().numpy()           # use detach when it's an output that requires_grad\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
